- name: pvp ovs-kernel DUT setup
  hosts: dut
  vars_files:
    - ./test_settings.yml
  tasks:
  - debug:
      msg: Debug mode is enabled
    when: redhat_debug_mode == true

############################################
# Get needed cpu list #
############################################

  - name: Get dut_isolated_cpus
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} dut_isolated_cpus
    register: dut_isolated_cpus
  - name: Get vcpu_0
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_0
    register: vcpu_0
  - name: Get vcpu_1
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_1
    register: vcpu_1
  - name: Get vcpu_2
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_2
    register: vcpu_2
  - name: Get vcpu_3
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_3
    register: vcpu_3
  - name: Get vcpu_emulator
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_emulator
    register: vcpu_emulator


#############################
# setting isolated CPUs #
# ###########################

  - name: Checking for isolcpus in grub (Fatal errors are normal here)
    shell: >
      grep "nohz_full={{ vcpu_0.stdout }},{{ vcpu_1.stdout }},{{ vcpu_2.stdout }},{{ vcpu_3.stdout }}" /proc/cmdline &&
      grep "rcu_nocbs={{ vcpu_0.stdout }},{{ vcpu_1.stdout }},{{ vcpu_2.stdout }},{{ vcpu_3.stdout }}" /proc/cmdline &&
      grep "isolcpus={{ vcpu_0.stdout }},{{ vcpu_1.stdout }},{{ vcpu_2.stdout }},{{ vcpu_3.stdout }}" /proc/cmdline
    failed_when: false
    ignore_errors: yes
    register: isolcpus_ok
  - block:
    - name: Adding isolated cpus to tuned
      lineinfile:
         path: //etc/tuned/cpu-partitioning-variables.conf
         regexp: "^isolated_cores=.*"
         line: "isolated_cores={{ vcpu_0.stdout }},{{ vcpu_1.stdout }},{{ vcpu_2.stdout }},{{ vcpu_3.stdout }}"
         state: present
    - name: Starting cpu-partitioning tuned profile
      shell: tuned-adm profile cpu-partitioning
    - name: restart service tuned
      systemd:
        name: tuned
        enabled: yes
        state: restarted
        daemon_reload: yes
    - name: Change isolcpus in grub
      shell: "grubby --args='isolcpus={{ vcpu_0.stdout }},{{ vcpu_1.stdout }},{{ vcpu_2.stdout }},{{ vcpu_3.stdout }}' --update-kernel=$(grubby --default-kernel)"

    - block: 
      - name: set BootNext for UEFI
        shell: |
          efibootmgr -v && efibootmgr -n $(efibootmgr -v | grep BootCurrent | sed 's/.*: //') || true
      - name: REBOOTING DUT SERVER NOW 
        reboot:
    when: isolcpus_ok.rc != 0

#######################################
# Release the DPDK NIC back to kernel #
# #####################################

  - name: Start openvswitch
    service:
      name: openvswitch
      state: started
  - name: Delete ovs bridge
    shell: ovs-vsctl --if-exists del-br ovs_pvp_br0
  - name: Disable dpdk
    shell: ovs-vsctl set Open_vSwitch . other_config:dpdk-init=false
  - name: Stop openvswitch
    service:
      name: openvswitch
      state: stopped
  - name: Unbind nic from dpdk
    shell: driverctl -v unset-override {{ dut_interface_1_pciid }}
  - name: enable nic
    shell: ip link set {{ dut_interface_1 }} up
  - name: restart service openvswitch
    systemd:
      name: openvswitch
      enabled: no
      state: restarted
      daemon_reload: yes

#################
# Re-config OVS #
#################

  - name: Recreate ovs
    shell: ovs-vsctl add-br ovs_pvp_br0
  - name: Add nic into ovs
    shell: ovs-vsctl add-port ovs_pvp_br0 {{ dut_interface_1 }} -- set Interface {{ dut_interface_1 }} ofport_request=1
  - name: Restart openvswitch
    service:
      name: openvswitch
      state: restarted

###################
# Create  VM #
# #################

  # remove all existing vms
  - name: list all VMs
    virt:
      command: list_vms
    register: all_vms
  - name: destroy all vms
    virt:
      name: "{{ item }}"
      command: destroy
    ignore_errors: True
    loop: "{{ all_vms.list_vms }}"
  - name: undefine all vms
    virt:
      name: "{{ item }}"
      command: undefine
    loop: "{{ all_vms.list_vms }}"

  #set static IP address 192.168.122.123 for internet access in guest g1
  - shell: virsh net-list | grep default || virsh net-define /usr/share/libvirt/networks/default.xml
  - shell: virsh net-autostart default
  - shell: virsh net-list | grep default | grep active || virsh net-start default
  - shell: >
      virsh net-dumpxml default | grep "mac='52:54:00:01:02:03'" &&
      virsh net-update default modify ip-dhcp-host "<host mac='52:54:00:01:02:03' ip='192.168.122.5'/>" --live --config ||
      virsh net-update default add ip-dhcp-host "<host mac='52:54:00:01:02:03' ip='192.168.122.5'/>" --live --config

  - name: create VM
    shell: virt-install --connect=qemu:///system --network bridge=ovs_pvp_br0,virtualport_type=openvswitch,model=virtio --network network=default,mac=52:54:00:01:02:03 --name=rhel_loopback_kerneldp --disk path=/opt/images/rhel_guest_image_pvp.qcow2,format=qcow2 --ram 8192 --memorybacking hugepages=on,size=1024,unit=M,nodeset=0,access_mode=shared --vcpus=4,cpuset={{ vcpu_0.stdout }},{{ vcpu_1.stdout }},{{ vcpu_2.stdout }},{{ vcpu_3.stdout }} --check-cpu --cpu mode=host-passthrough,+pdpe1gb,cell0.id=0,cell0.cpus=0,cell0.memory=8388608 --numatune mode=strict,nodeset=0 --nographics --noautoconsole --import

  - name: Wait vm to start
    pause:
      seconds: 60

  - name: Setting cpu pin for vcpu 0
    shell: virsh vcpupin rhel_loopback_kerneldp 0 {{ vcpu_0.stdout }}
  - name: Setting cpu pin for vcpu 1
    shell: virsh vcpupin rhel_loopback_kerneldp 1 {{ vcpu_1.stdout }}
  - name: Setting cpu pin for vcpu 2
    shell: virsh vcpupin rhel_loopback_kerneldp 2 {{ vcpu_2.stdout }}
  - name: Setting cpu pin for vcpu 3
    shell: virsh vcpupin rhel_loopback_kerneldp 3 {{ vcpu_3.stdout }}
  - name: Setting emulator pin for guest
    shell: virsh emulatorpin rhel_loopback_kerneldp {{ vcpu_emulator.stdout }}
  - name: copying virsh console script to default location
    copy:
      src: ./vm.sh
      dest: ~/vm.sh
      mode: "777"
  - name: Logging into vm rhel_loopback_kerneldp
    shell: ~/vm.sh login_vm rhel_loopback_kerneldp
  - name: Setting up subscription manager step 1 on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "subscription-manager register --username={{ rh_sub_username }} --password={{ rh_sub_pass }}"
    when: not qe_subscription_mode
  - name: Setting up subscription manager step 2 on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "subscription-manager attach --pool={{ rh_sub_pool_id }}"
    when: not qe_subscription_mode
  - name: Using QE Secret sauce to add subscription for repos
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "{{ qe_subscription_command }}"
    retries: 5
    delay: 3
    when: qe_subscription_mode
  - name: subscribing to appstream on VM rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "subscription-manager repos --enable rhel-8-for-x86_64-appstream-rpms || subscription-manager repos --enable rhel-8-for-x86_64-appstream*-rpms"
  - name: Running yum commandset step 1 on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "yum -y clean all"
  - name: Running yum commandset step 2 on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "yum -y update"
  - name: Running yum commandset step 3 on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "yum -y install driverctl gcc kernel-devel numactl-devel tuned-profiles-cpu-partitioning wget libibverbs dpdk-tools"
  - name: Running yum commandset step 4 on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "yum -y update kernel"
  - name: Modifying grub for tuning on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "grubby --args='isolcpus=1,2,3 default_hugepagesz=1G hugepagesz=1G hugepages=2' --update-kernel=ALL"
  - name: Setting vfio options on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "echo 'options vfio enable_unsafe_noiommu_mode=1' > /etc/modprobe.d/vfio.conf"
  - name: Binding nic to vfio-pci on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "driverctl -v set-override 0000:00:02.0 vfio-pci"
  - name: Starting tuned service step 1 on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "systemctl enable tuned"
  - name: Starting runed service step 2 on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "systemctl start tuned"
  - name: Setting isolated cores for tuning profile on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "grep -e '^isolated_cores=1,2,3' /etc/tuned/cpu-partitioning-variables.conf || echo isolated_cores=1,2,3 >> /etc/tuned/cpu-partitioning-variables.conf"
  - name: Starting tuned profile cpu-partitioning on rhel_loopback_kerneldp
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "tuned-adm profile cpu-partitioning"
  - name: Rebooting vm rhel_loopback_kerneldp step 1
    virt:
      name: rhel_loopback_kerneldp
      state: shutdown
  - name: Wait system some time to shutdown
    pause:
      seconds: 5
  - name: Rebooting vm rhel_loopback_kerneldp step 2
    virt:
      name: rhel_loopback_kerneldp
      state: running
  - name: Wait vm to start
    pause:
      seconds: 60
  - name: Logging into VM rhel_loopback_kerneldp
    shell: ~/vm.sh login_vm rhel_loopback_kerneldp
  - name: Logging into vm rhel_loopback_kerneldp
    shell: ~/vm.sh login_vm rhel_loopback_kerneldp
  - name: Getting IP address from VM
    shell: ~/vm.sh run_cmd rhel_loopback_kerneldp "ip a | grep inet | grep brd | awk '{ print \$2 }'" | grep -A 1 "ip a \| grep inet \| grep brd"   | tail -n 1
    register: vm_ip
  - debug:
      msg: Please note the ip address of the VM as it will be needed when executing the pvp test scripts {{ vm_ip.stdout }}

