- name: pvp_ovsdpdk setup
  hosts: dut
  vars:
    guest_name: rhel_loopback
  vars_files:
    - ./test_settings.yml
  tasks:
  - debug:
      msg: Debug mode is enabled
    when: redhat_debug_mode == true

######################################
# Install repositories on DUT server #
######################################
  - rpm_key:
      state: present
      key: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-8  
  - name: Install FedoraProject Repo
    yum:
      name: https://dl.fedoraproject.org/pub/epel/epel-release-latest-{{ ansible_distribution_major_version }}.noarch.rpm
      state: present
      
  - name: Install needed repos stage 1
    yum:
      name: wget,aspell,aspell-en,autoconf,automake,bc,checkpolicy
  - name: Install needed repos stage 2
    yum:
      name: desktop-file-utils,dpdk,dpdk-tools,driverctl,emacs,expect,gcc,gcc-c++,gdb
  - name: Install needed repos stage 3
    yum:
      name: git,graphviz,hwloc,intltool,libcap-ng
  - name: Install needed repos stage 4
    yum:
      name: libcap-ng-devel,libguestfs,libguestfs-tools-c,libtool,libvirt
  - name: Install needed repos stage 5
    yum:
      name: lshw,openssl,openssl-devel,procps-ng,python3
  - name: Install needed repos stage 6
    yum:
      name: python3-six,rpm-build,selinux-policy-devel,sshpass,sysstat
  - name: Install needed repos stage 7
    yum:
      name: systemd-units,tcpdump,time,tmux,tuned-profiles-cpu-partitioning
  - name: Install needed repos stage 8
    yum:
      name: virt-install,virt-manager,wget
  - name: Installing openvswitch packages
    yum:
      name: "{{ ovs_selinux_rpm_path }}, {{ ovs_rpm_path }}"
      disable_gpg_check: yes
    when: redhat_debug_mode == true
  - name: Create symbolic link for python3
    file:
      src: "/usr/bin/python3"
      dest: "/usr/bin/python"
      state: link
  - name: Create symbolic link for pip3
    file:
      src: "/usr/bin/pip3"
      dest: "/usr/bin/pip"
      state: link
  - name: copying vm.sh to default location
    copy:
      src: ./vm.sh
      dest: ~/vm.sh
      mode: "777"
  - name: Copy up cpulist script to remote DUT
    copy:
      src: get_cpulist.sh
      dest: /root/get_cpulist.sh
      mode: "a+x"

####################################################
# Tweak the system for QEMU and and OVS-DPDK usage #
####################################################
  - name: Modifying selinux params step 1
    lineinfile:
      path: /etc/selinux/config
      regexp: '^SELINUX=.*'
      line: "SELINUX=permissive"
      state: present
  - name: Modifying selinux params step 2
    shell: setenforce permissive

################################
# Starting libvirtd service #
################################
  - name: group = 'hugetlbfs' in /etc/libvirt/qemu.conf
    lineinfile:
      path: /etc/libvirt/qemu.conf
      regexp: '^group[[:space:]]*=.*'
      line: "group = 'hugetlbfs'"
      state: present
    register: result_hugetlbfs
  
  - name: restart service libvirtd
    systemd:
      name: libvirtd
      enabled: yes
      state: restarted
      daemon_reload: yes
    when: result_hugetlbfs.changed

################################
# Starting openvswitch service #
################################
  - name: Enable service openvswitch
    service:
      name: openvswitch
      enabled: yes
  - name: Start service openvswitch
    service:
      name: openvswitch
      state: started

################################
# Removing old configurations if exists
################################
  - name: list all VMs
    virt:
      command: list_vms
    register: all_vms
  - virt:
      name: "{{ item }}"
      state: destroyed
    loop: "{{ all_vms.list_vms }}"
  - pause:
      seconds: 5
  - virt:
      name: "{{ item }}"
      command: undefine
    loop: "{{ all_vms.list_vms }}"

  - name: ovs-vsctl del-br
    shell: ovs-vsctl list-br | awk '{ system("ovs-vsctl del-br "$1) }'
  - name: stop service openvswitch
    systemd:
      name: openvswitch
      state: stopped

  - name: driverctl unset_overrides
    shell: driverctl list-overrides 2>/dev/null | awk '{ system("driverctl unset-override "$1) }'

  - name: remove /dev/hugepages/rtemap_*
    file:
      path: /dev/hugepages/rtemap_*
      state: absent

#####################################################
# Modify startup parameters for hugepages and iommu
#####################################################
  - name: Checking required cmdline
    shell: "grep '{{ dut_linux_cmdline }}' /proc/cmdline"
    register: dut_linux_cmdline_found
    failed_when: dut_linux_cmdline_found.rc > 1
  - debug: var=dut_linux_cmdline_found
  - name: Tweak the kernel for dut_linux_cmdline
    shell: "grubby --args='{{ dut_linux_cmdline }}' --update-kernel=$(grubby --default-kernel)"
    when: dut_linux_cmdline_found.rc != 0

############################################
# Get needed cpu list #
############################################
  - name: check cpu requirement
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} dut_isolated_cpus
    register: cpu_meet
  - block:
    - debug:
        msg: cpu list can't meet the test requirement,exit the script
    - meta: end_play
    when: cpu_meet.rc != 0
  - name: Get dut_isolated_cpus
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} dut_isolated_cpus
    register: dut_isolated_cpus
  - name: Get dut_dpdk_pmd_mask
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} dut_dpdk_pmd_mask
    register: dut_dpdk_pmd_mask
  - name: Get dut_pmd_rxq_affinity
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} dut_pmd_rxq_affinity
    register: dut_pmd_rxq_affinity
  - name: Get vcpu_0
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_0
    register: vcpu_0
  - name: Get vcpu_1
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_1
    register: vcpu_1
  - name: Get vcpu_2
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_2
    register: vcpu_2
  - name: Get vcpu_3
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_3
    register: vcpu_3
  - name: Get dut_dpdk_lcore_mask
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} dut_dpdk_lcore_mask
    register: dut_dpdk_lcore_mask
  - name: Get vcpu_emulator
    shell: ./get_cpulist.sh {{ dut_interface_1_pciid }} vcpu_emulator
    register: vcpu_emulator

############################################
# Tune isolated cpus
############################################
  - name: Checking for isolcpus in grub (Fatal errors are normal here)
    shell: >
      grep "nohz_full={{ dut_isolated_cpus.stdout }}" /proc/cmdline &&
      grep "rcu_nocbs={{ dut_isolated_cpus.stdout }}" /proc/cmdline
    register: isolcpus_needed
    failed_when: isolcpus_needed.rc > 1
  - name: Adding isolated cpus to tuned
    lineinfile:
       path: //etc/tuned/cpu-partitioning-variables.conf
       regexp: "^isolated_cores=.*"
       line: "isolated_cores={{ dut_isolated_cpus.stdout }}"
       state: present
  - name: Starting cpu-partitioning tuned profile
    shell: tuned-adm profile cpu-partitioning
  - name: restart service tuned
    systemd:
      name: tuned
      enabled: yes
      state: restarted
      daemon_reload: yes

###################################################
# Reboot DUT system for settings to take effect #
###################################################
  - name: set BootNext for UEFI system
    shell: >
      efibootmgr -v && efibootmgr -n $(efibootmgr -v | grep BootCurrent | sed 's/.*: //')
    register: re
    failed_when:
      - re.rc != 0
      - '"efibootmgr: command not found" not in re.stderr'

  - name: reboot
    reboot:
    when: dut_linux_cmdline_found.rc != 0 or isolcpus_needed.rc !=0

  - name: check hugepage
    shell: grep -i huge /proc/meminfo

####################
# Getting NIC info #
####################

  - name: Binding nic to {{ dut_driver_override }}
    shell: |
      driverctl -v set-override {{ dut_interface_1_pciid }} {{ dut_driver_override }}
      driverctl -v set-override {{ dut_interface_2_pciid }} {{ dut_driver_override }}
    when: dut_driver_override != ''

################################
# Starting openvswitch service #
################################

  - name: Enable service openvswitch
    service:
      name: openvswitch
      enabled: yes
  - name: Start service openvswitch, if not started
    service:
      name: openvswitch
      state: started

############################
# Starting OVS-DPDK config #
############################

  - name: Setting up OVS-DPDK config step 1
    shell: "ovs-vsctl set Open_vSwitch . other_config:dpdk-init=true"
  - name: Setting up OVS-DPDK config step 2
    shell: ovs-vsctl set Open_vSwitch . other_config:dpdk-socket-mem=4096
  - name: Setting up OVS-DPDK config step 3
    shell: ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask={{ dut_dpdk_pmd_mask.stdout }}
  - name: Setting up OVS-DPDK config step 4
    shell: ovs-vsctl set Open_vSwitch . other_config:dpdk-lcore-mask={{ dut_dpdk_lcore_mask.stdout }}
  - name: Restart service openvswitch
    service:
      name: openvswitch
      state: restarted
  - name: Setting up OVS-DPDK config step 5
    shell: ovs-vsctl --if-exists del-br ovs_pvp_br0
  - name: Setting up OVS-DPDK config step 6
    shell: ovs-vsctl add-br ovs_pvp_br0 -- set bridge ovs_pvp_br0 datapath_type=netdev
  - name: Setting up OVS-DPDK config step 7
    shell: >
      ovs-vsctl add-port ovs_pvp_br0 dpdk0 -- set Interface dpdk0 type=dpdk
      options:dpdk-devargs={{ dut_interface_1_pciid }}
      options:n_rxq=2
      other_config:pmd-rxq-affinity={{ dut_pmd_rxq_affinity.stdout }}
      ofport_request=1
  - name: Setting up OVS-DPDK config step 8
    shell: >
      ovs-vsctl add-port ovs_pvp_br0 vhost0 -- set Interface vhost0 type=dpdkvhostuserclient
      options:vhost-server-path='/tmp/vhost-sock0'
      options:n_rxq=2
      other_config:pmd-rxq-affinity={{ dut_pmd_rxq_affinity.stdout }}
      ofport_request=2

##############################
# Setting up Virtual machine #
##############################
  - stat: path=/opt/images
    register: st
  - name: Creates directory for image storage
    file:
      path: /opt/images
      state: directory
    when: not st.stat.exists
  - stat: path=/opt/images/{{ rhel_guest_image_path | basename }}
    register: st
  - name: Copy image to /opt/images
    copy:
      src: "{{ rhel_guest_image_path }}"
      dest: /opt/images/rhel_guest_image_pvp.qcow2
      remote_src: yes
    when: not st.stat.exists
  - name: Configuring VM
    shell: >
      LIBGUESTFS_BACKEND=direct virt-customize
      -a /opt/images/rhel_guest_image_pvp.qcow2
      --root-password password:root
      --uninstall cloud-init

###################################################
# set static IP address 192.168.122.123 for internet access in guest
###################################################
  - name: destroy virt-net default
    virt_net:
      name: default
      command: destroy
    failed_when: false
  - name: undefine virt-net default
    virt_net:
      name: default
      command: undefine
  - name: define virt-net default
    virt_net:
      name: default
      command: define
      xml: "{{ lookup('file', '/usr/share/libvirt/networks/default.xml') }}"
      autostart: yes
  - name: set IP address for internet access in guest
    virt_net:
      name: default
      command: modify
      xml: "<host name='{{ guest_name }}' mac='52:54:00:01:02:03' ip='192.168.122.5'/>"
  - name: start virt-net default
    virt_net:
      name: default
      command: create
  - name: check virt-net default
    virt_net:
      name: default
      command: get_xml

  - name: Running virt-install
    shell: >
      virt-install --connect=qemu:///system
      --name={{ guest_name }}
      --check-cpu
      --cpu mode=host-passthrough,+pdpe1gb,cell0.id=0,cell0.cpus=0,cell0.memory=8388608
      --vcpus=4,cpuset={{ vcpu_0.stdout }},{{ vcpu_1.stdout }},{{ vcpu_2.stdout }},{{ vcpu_3.stdout }}
      --ram 8192
      --numatune mode=strict,nodeset=0
      --memorybacking hugepages=on,size=1024,unit=M,nodeset=0,access_mode=shared
      --network vhostuser,source_type=unix,source_path=/tmp/vhost-sock0,source_mode=server,model=virtio,driver_queues=2
      --network network=default,mac=52:54:00:01:02:03
      --disk path=/opt/images/rhel_guest_image_pvp.qcow2,format=qcow2
      --nographics
      --noautoconsole
      --import
  - name: Wait vm to start
    pause:
      seconds: 60
  - name: Setting permissions on vhost socket folder
    file:
       path: /tmp/vhost-sock0
       mode: 777
  - name: Setting cpu pin for vcpu
    shell: |
      virsh vcpupin {{ guest_name }} 0 {{ vcpu_0.stdout }}
      virsh vcpupin {{ guest_name }} 1 {{ vcpu_1.stdout }}
      virsh vcpupin {{ guest_name }} 2 {{ vcpu_2.stdout }}
      virsh vcpupin {{ guest_name }} 3 {{ vcpu_3.stdout }}
      virsh emulatorpin {{ guest_name }} {{ vcpu_emulator.stdout }}

  - name: Logging into vm {{ guest_name }}
    shell: ~/vm.sh login_vm {{ guest_name }}

  - name: Setting up subscription manager step 1 on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "subscription-manager register --username={{ rh_sub_username }} --password={{ rh_sub_pass }}"
    when: not qe_subscription_mode
  - name: Setting up subscription manager step 2 on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "subscription-manager attach --pool={{ rh_sub_pool_id }}"
    when: not qe_subscription_mode

  - name: Using QE Secret sauce to add subscription for repos
    shell: ~/vm.sh run_cmd {{ guest_name }} "{{ qe_subscription_command }}"
    retries: 5
    delay: 3
    when: qe_subscription_mode

  - name: subscribing to appstream on VM {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "subscription-manager repos --enable rhel-8-for-x86_64-appstream-rpms || subscription-manager repos --enable rhel-8-for-x86_64-appstream*-rpms"
  - name: Running yum commandset step 1 on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "yum -y clean all"
  #- name: Running yum commandset step 2 on {{ guest_name }}
  #  shell: ~/vm.sh run_cmd {{ guest_name }} "yum -y update"
  - name: Running yum commandset step 3 on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "yum -y install driverctl gcc kernel-devel numactl-devel tuned-profiles-cpu-partitioning wget libibverbs dpdk-tools"
  #- name: Running yum commandset step 4 on {{ guest_name }}
  #  shell: ~/vm.sh run_cmd {{ guest_name }} "yum -y update kernel"

  - name: Modifying grub for tuning on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "grubby --args='isolcpus=1,2,3 default_hugepagesz=1G hugepagesz=1G hugepages=2' --update-kernel=ALL"
  - name: Setting vfio options on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "echo 'options vfio enable_unsafe_noiommu_mode=1' > /etc/modprobe.d/vfio.conf"
  - name: Binding nic to vfio-pci on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "driverctl -v set-override 0000:00:02.0 {{ vm_driver_override }}"
    when: vm_driver_override != ""
  - name: Starting tuned service step 1 on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "systemctl enable tuned"
  - name: Starting runed service step 2 on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "systemctl start tuned"
  - name: Setting isolated cores for tuning profile on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "grep -e '^isolated_cores=1,2,3' /etc/tuned/cpu-partitioning-variables.conf || echo isolated_cores=1,2,3 >> /etc/tuned/cpu-partitioning-variables.conf"
  - name: Starting tuned profile cpu-partitioning on {{ guest_name }}
    shell: ~/vm.sh run_cmd {{ guest_name }} "tuned-adm profile cpu-partitioning"

  - name: Rebooting vm {{ guest_name }} step 1
    virt:
      name: "{{ guest_name }}"
      state: shutdown
  - name: Wait system some time to shutdown
    pause:
      seconds: 5
  - name: Rebooting vm {{ guest_name }} step 2
    virt:
      name: "{{ guest_name }}"
      state: running
  - name: Wait vm to start
    pause:
      seconds: 60
  - name: Logging into VM {{ guest_name }}
    shell: ~/vm.sh login_vm {{ guest_name }}
  - name: Logging into vm {{ guest_name }}
    shell: ~/vm.sh login_vm {{ guest_name }}
  - name: Getting IP address from VM
    shell: ~/vm.sh run_cmd {{ guest_name }} "ip a | grep inet | grep brd | awk '{ print \$2 }'" | grep -A 1 "ip a \| grep inet \| grep brd"   | tail -n 1
    register: vm_ip
  - debug:
      msg: Please note the ip address of the VM as it will be needed when executing the pvp test scripts {{ vm_ip.stdout }}
